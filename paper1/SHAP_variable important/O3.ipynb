{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap\n",
    "#!pip install sklearn\n",
    "#!pip install astunparse\n",
    "import shap\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qualifying here.... load model and predict\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import concatenate\n",
    "print(tf.__version__)\n",
    "#fix with tf 2.1\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LSTM 16- 1 layer - timestep =1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder    \n",
    "\n",
    "# source https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def Fill_data(df,mean,timesteps,n_features):\n",
    "    columns = df.columns\n",
    "    row_stand = df.iloc[0]\n",
    "    for i in range (1,timesteps):\n",
    "        row_temp = np.array([])\n",
    "        for j in range(0,4):\n",
    "            temp = row_stand[j]-i\n",
    "            row_temp = np.append(row_temp,temp)\n",
    "        for j in range(4,n_features):\n",
    "            row_temp = np.append(row_temp,mean[j])\n",
    "\n",
    "        minus = 0\n",
    "        if(row_temp[0]<0): #index\n",
    "            row_temp[0] = 0\n",
    "        if(row_temp[1]<0): #hour\n",
    "            row_temp[1] += 24\n",
    "            minus = 1\n",
    "        if(row_temp[2]<0 or minus == 1): #day of the year\n",
    "            row_temp[2] += 365\n",
    "            if(minus == 1):\n",
    "                row_temp[2] -= 1\n",
    "                minus= 0\n",
    "        if(row_temp[3]<0): #weekday : from 0 to 6\n",
    "            row_temp[4] = 0\n",
    "        row_temp= pd.DataFrame(row_temp.reshape(1, len(row_temp)),columns=columns)\n",
    "        df = pd.concat([row_temp,df],ignore_index=True,axis=0)\n",
    "    return df\n",
    "\n",
    "def Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    y_real = concatenate((X_test,y_real), axis=1)\n",
    "    y_real = scaler.inverse_transform(y_real)\n",
    "    y_real = y_real[:,-1]\n",
    "\n",
    "    RMSE = sqrt(mean_squared_error(y_real, y_pred))\n",
    "    from sklearn.metrics import r2_score\n",
    "    R2 = r2_score(y_real, y_pred)\n",
    "    return RMSE,R2\n",
    "\n",
    "def Evaluation2(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def Quatifying(nodes,timesteps,data_path,Train_file,Test_file, Model_name,y_name,Output_file,original_data):\n",
    "\n",
    "    #load model from file - the best model\n",
    "    loaded_model = load_model(Model_name)\n",
    "\n",
    "    #compile the model\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    #loaded_model.compile(loss = 'mse',\n",
    "    #     optimizer = opt, \n",
    "    #     metrics = ['mean_squared_error'])\n",
    "\n",
    "    #Load X_test, y_real\n",
    "    data = pd.read_csv(data_path) #four year data\n",
    "    data = data.dropna()\n",
    "    Train1 = pd.read_csv(Train_file)\n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = pd.read_csv(Test_file)\n",
    "    Test1 = Test1.dropna()\n",
    "    \n",
    "    New_data = [Train1, Test1]\n",
    "    New_data = pd.concat(New_data)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "    Train =New_data[0:Train1.shape[0],:]\n",
    "    Test = New_data[Train1.shape[0]:,:]\n",
    "    \n",
    "    Quatify = data[data['Index']>35424 ] #get data from 15/1/2020\n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "    #padding because of timesteps\n",
    "    Quatifying = Fill_data(Quatify,mean,timesteps, n_features )\n",
    "\n",
    "    #tranform train and test.\n",
    "    New_data = Quatifying.values\n",
    "\n",
    "    New_data = scaler.transform(New_data)\n",
    "\n",
    "    X_Quatifying, y_real = split_sequences(New_data, timesteps, 1)\n",
    "\n",
    "    print( 'size after appling timesteps: ',X_Quatifying.shape,y_real.shape)\n",
    "\n",
    "    #Evaluate\n",
    "    y_pred = Evaluation2(loaded_model,X_Quatifying,y_real,scaler,timesteps,n_features)\n",
    "    \n",
    "    df = pd.DataFrame(Quatify)\n",
    "    cols = list(Quatify.columns)\n",
    "    cols = [cols[0], cols[-1]]\n",
    "    df3 = df[cols]\n",
    "    columnName = df3.columns\n",
    "\n",
    "    \n",
    "\n",
    "    df3['Predicted']= y_pred\n",
    "\n",
    "\n",
    "    data_original = pd.read_excel(original_data, 'Sheet1')\n",
    "    data_original= data_original[data_original['FECHA_UNIDAD']>='2016-01-01 00:00:00']\n",
    "    Features = data_original.iloc[:,6:]\n",
    "    FECHA_UNIDAD =  data_original[['FECHA_UNIDAD']]\n",
    "\n",
    "    FECHA_UNIDAD.insert(0, 'Index', np.arange(1,len(FECHA_UNIDAD[:])+1))\n",
    "\n",
    "    Target = data_original[[y_name]]\n",
    "    Target = pd.to_numeric(Target[y_name], errors='coerce')\n",
    "    Target_data = pd.concat([FECHA_UNIDAD,Features,Target], axis=1)\n",
    "    Target_data = Target_data.rename(columns={'FECHA_UNIDAD': 'DateTime'})\n",
    "    Target_data = Target_data.dropna()\n",
    "    Target_data = Target_data[Target_data['DateTime']> '2020-01-15 23:00:00']\n",
    "    Target_data['Predicted']= df3['Predicted'].values\n",
    "\n",
    "    Target_data.to_csv(Output_file,index=False)\n",
    "\n",
    "    print('\\nDone wtire to file...',Output_file,'\\n')\n",
    "    #return Quatify,y_pred #dataframe include X, and predict value.\n",
    "    \n",
    "def Preprocess(nodes,timesteps,data_path,Train_file,Test_file, Model_name,y_name,Output_file,original_data):\n",
    "\n",
    "    #load model from file - the best model\n",
    "    loaded_model = load_model(Model_name)\n",
    "\n",
    "    #Load X_test, y_real\n",
    "    data = pd.read_csv(data_path) #four year data\n",
    "    data = data.dropna()\n",
    "    Train1 = pd.read_csv(Train_file)\n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = pd.read_csv(Test_file)\n",
    "    Test1 = Test1.dropna()\n",
    "    \n",
    "    New_data = [Train1, Test1]\n",
    "    New_data = pd.concat(New_data)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "    Train =New_data[0:Train1.shape[0],:]\n",
    "    Test = New_data[Train1.shape[0]:,:]\n",
    "    \n",
    "    Training = Train1\n",
    "    \n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "    #padding because of timesteps\n",
    "    Training = Fill_data(Training,mean,timesteps, n_features )\n",
    "    Testing = Fill_data(Test1,mean,timesteps, n_features )\n",
    "\n",
    "    #tranform train and test.\n",
    "    New_data = Training.values\n",
    "    Testing  = Testing.values\n",
    "\n",
    "    New_data = scaler.transform(New_data)\n",
    "    New_test =  scaler.transform(Testing)\n",
    "\n",
    "    X_Training, y_training = split_sequences(New_data, timesteps, 1)\n",
    "    X_Testing, y_testing = split_sequences(New_test, timesteps, 1)\n",
    "\n",
    "    print( 'size after appling timesteps: ',X_Training.shape,y_training.shape)\n",
    "    print ('done')\n",
    "    return loaded_model, X_Training, y_training, X_Testing, y_testing, scaler\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CO_mg/m3\n",
    "#SO2_ug/m3\n",
    "#PM2.5_ug/m3\n",
    "#O3_ug/m3\n",
    "nodes=64\n",
    "timesteps=3\n",
    "data_path = 'C:\\\\data\\\\4_years_data\\\\O3.csv'\n",
    "Model_name = 'C:\\\\data\\\\best_model\\\\O3_best.h5'\n",
    "Train_file = 'C:\\\\data\\\\Data_paper\\\\O3_Train.csv'\n",
    "Test_file = 'C:\\\\data\\\\Data_paper\\\\O3_Test.csv'\n",
    "original_data = 'C:\\\\data\\\\Data_belisario2.xlsx'\n",
    "#Output_file = 'PM2_5_Results_LSTM_96_12_Quatifying.csv'\n",
    "y_name = 'O3_mg/m3'\n",
    "Output_file = 'C:\\\\data\\\\SHAP\\\\O3_Results_Quatifying.csv'\n",
    "\n",
    "loaded_model, X_Training, y_training, X_Testing, y_testing, scaler = Preprocess(nodes,timesteps,data_path,Train_file,Test_file, Model_name,y_name,Output_file,original_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = loaded_model.predict(X_Training)\n",
    "print(np.shape(temp))\n",
    "print(np.shape(X_Training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import tensorflow as tf\n",
    "print (shap.__version__)\n",
    "print (tf.__version__)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/datadriveninvestor/time-step-wise-feature-importance-in-deep-learning-using-shap-e1c46a655455\n",
    "#simpler NN https://christophm.github.io/interpretable-ml-book/neural-networks.html \n",
    "#this is under development.\n",
    "#error: https://stackoverflow.com/questions/64401570/error-using-shap-with-simplernn-sequential-model\n",
    "import shap\n",
    "import tensorflow.keras.backend \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import shap\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#model = load_model(Model_name)\n",
    "\n",
    "#pred_x = regressor.predict_classes(X_Training)\n",
    "\n",
    "#shap.explainers.deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers.deep.deep_tf.passthrough\n",
    "#explainer  = shap.DeepExplainer((regressor.layers[0].input, regressor.layers[-1].output),data)\n",
    "#background = X_Training[np.random.choice(X_Training.shape[0], 200, replace=False)]\n",
    "\n",
    "#from shap.explainers.deep.deep_tf import passthrough\n",
    "#shap.explainers._deep.deep_tf.op_handlers['AddV2'] = passthrough\n",
    "#explainer  = shap.DeepExplainer(regressor,background)\n",
    "\n",
    "background = X_Training[np.random.choice(X_Training.shape[0], 15000, replace=False)]\n",
    "#test = X_Training[np.random.choice(X_Training.shape[0], 300, replace=False)]\n",
    "explainer = shap.DeepExplainer(loaded_model, background)\n",
    "shap_vals = explainer.shap_values(X_Testing)\n",
    "f_names = ['Index','Hours','Julian_day','Week_day','Temperature','SR','p','Prec','RH','WD','WS']\n",
    "shap.summary_plot(shap_vals[0][:, timesteps-1, :],feature_names=f_names,  plot_type=\"bar\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
