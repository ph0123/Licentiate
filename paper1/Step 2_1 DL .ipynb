{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is run 3 folds, and 5 times for each data and each model.\n",
    "#note that: do not save the model. \n",
    "#saved model is in next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layers.\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "#print(tf.__version__)\n",
    "#fix with tf 2.1\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def My_model(nodes, X_train, y_train,X_test, y_real,model_name):\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=20,\n",
    "                    restore_best_weights =True)\n",
    "    #mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #                         filepath=model_name, \n",
    "    #                         monitor='val_loss', \n",
    "    #                         save_best_only=True)\n",
    "    #the best model will be used, if you want to store the model, uncomment above.\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "                             filepath=model_name,\n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True)\n",
    "    callback = [es,mc]\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "    my_model = tf.keras.Sequential()\n",
    "    my_model.add(tf.keras.layers.LSTM(units = nodes,input_shape = (X_train.shape[1],X_train.shape[2])))\n",
    "    my_model.add(tf.keras.layers.Dropout(0.25))\n",
    "    my_model.add(tf.keras.layers.Dense(1))\n",
    "    my_model.compile(loss = 'mse',\n",
    "     optimizer = opt, \n",
    "     metrics = ['mean_squared_error'])\n",
    "\n",
    "    #my_model.summary()\n",
    "    my_model.fit(X_train,y_train,\n",
    "                    validation_data=(X_test, y_real),\n",
    "                    batch_size = 500,epochs = 300,\n",
    "                    callbacks=callback, verbose=0)\n",
    "    return my_model\n",
    "# source https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def Fill_data(df,mean,timesteps,n_features):\n",
    "    columns = df.columns\n",
    "    row_stand = df.iloc[0]\n",
    "    for i in range (1,timesteps):\n",
    "        row_temp = np.array([])\n",
    "        for j in range(0,4):\n",
    "            temp = row_stand[j]-i\n",
    "            row_temp = np.append(row_temp,temp)\n",
    "        for j in range(4,n_features):\n",
    "            row_temp = np.append(row_temp,mean[j])\n",
    "\n",
    "        minus = 0\n",
    "        if(row_temp[0]<0): #index\n",
    "            row_temp[0] = 0\n",
    "        if(row_temp[1]<0): #hour\n",
    "            row_temp[1] += 24\n",
    "            minus = 1\n",
    "        if(row_temp[2]<0 or minus == 1): #day of the year\n",
    "            row_temp[2] += 365\n",
    "            if(minus == 1):\n",
    "                row_temp[2] -= 1\n",
    "                minus= 0\n",
    "        if(row_temp[3]<0): #weekday : from 0 to 6\n",
    "            row_temp[4] = 0\n",
    "        row_temp= pd.DataFrame(row_temp.reshape(1, len(row_temp)),columns=columns)\n",
    "        df = pd.concat([row_temp,df],ignore_index=True,axis=0)\n",
    "    return df\n",
    "\n",
    "def Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    y_real = concatenate((X_test,y_real), axis=1)\n",
    "    y_real = scaler.inverse_transform(y_real)\n",
    "    y_real = y_real[:,-1]\n",
    "\n",
    "    RMSE = sqrt(mean_squared_error(y_real, y_pred))\n",
    "    from sklearn.metrics import r2_score\n",
    "    R2 = r2_score(y_real, y_pred)\n",
    "    return RMSE,R2\n",
    "def Plot_evaluation(my_model):\n",
    "    #show evaluation early stop\n",
    "    print(len(my_model.history.history['loss']))\n",
    "    plt.plot(my_model.history.history['loss'], label='train')\n",
    "    plt.plot(my_model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def Run(Train_path, Test_path):\n",
    "\n",
    "    Train1 = pd.read_csv(Train_path)\n",
    "    Test1 = pd.read_csv(Test_path)\n",
    "    \n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = Test1.dropna()\n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    print(Test1.shape)\n",
    "    print(Train1.shape)\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "\n",
    "    List_nodes =[16,32,48,64,96,128]\n",
    "    List_TimeSteps = [1,3,6,9,12]\n",
    "    #nodes = 16\n",
    "    #timesteps=1 \n",
    "    for nodes in List_nodes:\n",
    "        for timesteps in List_TimeSteps:\n",
    "\n",
    "            #padding because of timesteps\n",
    "            Train = Fill_data(Train1,mean,timesteps, n_features )\n",
    "            Test  = Fill_data(Test1,mean,timesteps, n_features )\n",
    "\n",
    "            #tranform train and test.\n",
    "            New_data = [Train,Test]\n",
    "            New_data = pd.concat(New_data)\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "            Train =New_data[0:Train.shape[0],:]\n",
    "            Test = New_data[Train.shape[0]:,:]\n",
    "\n",
    "            #print('LSTM nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            X_train, y_train = split_sequences(Train, timesteps, 1)\n",
    "            X_test, y_real = split_sequences(Test, timesteps, 1)\n",
    "\n",
    "            print( 'size after appling timesteps: ',X_train.shape,y_train.shape,X_test.shape, y_real.shape)\n",
    "\n",
    "\n",
    "            #LSTM,\n",
    "            my_model = My_model(nodes, X_train, y_train,X_test, y_real,\n",
    "                                '3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n",
    "            print('model stop after ',len(my_model.history.history['loss']), ' epochs')\n",
    "\n",
    "            # Plot evaluation\n",
    "            #Plot_evaluation(my_model)\n",
    "\n",
    "            #Evaluate\n",
    "            RMSE, R2 = Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features)\n",
    "\n",
    "            print('LSTM nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            print('RMSE: ', RMSE)\n",
    "            print('R2: ', R2)\n",
    "            print('\\n\\n')\n",
    "            import os\n",
    "            os.remove('3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'NO2_Train.csv',data_path+'NO2_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'CO_Train.csv',data_path+'CO_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'O3_Train.csv',data_path+'O3_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'PM2_5_Train.csv',data_path+'PM2_5_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'SO2_Train.csv',data_path+'SO2_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "#CNN\n",
    "# test with two, 3 LSTM layers.\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "#thebest is LSTM with 16 nodes - 1 layer\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "#print(tf.__version__)\n",
    "#fix with tf 2.1\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "#process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "#LSTM 16- 1 layer - timestep =1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def My_model(nodes, X_train, y_train,X_test, y_real,kernel_size_, pool_size_,model_name):\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=20,\n",
    "                    restore_best_weights =True)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "                             filepath=model_name, \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True)\n",
    "    #the best model will be used, if you want to store the model, uncomment above.\n",
    "\n",
    "    callback = [es,mc]\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "    my_model = tf.keras.Sequential()\n",
    "    my_model.add(tf.keras.layers.Conv1D(filters = nodes,\n",
    "                                    kernel_size=kernel_size_,\n",
    "                                    input_shape =\n",
    "                                    (X_train.shape[1],\n",
    "                                     X_train.shape[2])))\n",
    "    my_model.add(tf.keras.layers.MaxPooling1D(pool_size=pool_size_))\n",
    "    my_model.add(tf.keras.layers.Flatten())\n",
    "    my_model.add(tf.keras.layers.Dropout(0.25))\n",
    "    my_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    my_model.compile(loss = 'mse',\n",
    "     optimizer = opt, \n",
    "     metrics = ['mean_squared_error'])\n",
    "\n",
    "    #my_model.summary()\n",
    "    my_model.fit(X_train,y_train,\n",
    "                    validation_data=(X_test, y_real),\n",
    "                    batch_size = 500,epochs = 300,\n",
    "                    callbacks=callback, verbose=0)\n",
    "    return my_model\n",
    "# source https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in):\n",
    "\tn_steps_out=1\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseqy = sequences[i:end_ix, -n_steps_out:]\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], seqy[-1,:]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "def Fill_data(df,mean,timesteps,n_features):\n",
    "    columns = df.columns\n",
    "    row_stand = df.iloc[0]\n",
    "    for i in range (1,timesteps):\n",
    "        row_temp = np.array([])\n",
    "        for j in range(0,4):\n",
    "            temp = row_stand[j]-i\n",
    "            row_temp = np.append(row_temp,temp)\n",
    "        for j in range(4,n_features):\n",
    "            row_temp = np.append(row_temp,mean[j])\n",
    "\n",
    "        minus = 0\n",
    "        if(row_temp[0]<0): #index\n",
    "            row_temp[0] = 0\n",
    "        if(row_temp[1]<0): #hour\n",
    "            row_temp[1] += 24\n",
    "            minus = 1\n",
    "        if(row_temp[2]<0 or minus == 1): #day of the year\n",
    "            row_temp[2] += 365\n",
    "            if(minus == 1):\n",
    "                row_temp[2] -= 1\n",
    "                minus= 0\n",
    "        if(row_temp[3]<0): #weekday : from 0 to 6\n",
    "            row_temp[4] = 0\n",
    "        row_temp= pd.DataFrame(row_temp.reshape(1, len(row_temp)),columns=columns)\n",
    "        df = pd.concat([row_temp,df],ignore_index=True,axis=0)\n",
    "    return df\n",
    "\n",
    "def Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    y_real = concatenate((X_test,y_real), axis=1)\n",
    "    y_real = scaler.inverse_transform(y_real)\n",
    "    y_real = y_real[:,-1]\n",
    "\n",
    "    RMSE = sqrt(mean_squared_error(y_real, y_pred))\n",
    "    from sklearn.metrics import r2_score\n",
    "    R2 = r2_score(y_real, y_pred)\n",
    "    return RMSE,R2\n",
    "def Plot_evaluation(my_model):\n",
    "    #show evaluation early stop\n",
    "    print(len(my_model.history.history['loss']))\n",
    "    plt.plot(my_model.history.history['loss'], label='train')\n",
    "    plt.plot(my_model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#load train and test data here\n",
    "##process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "def Run(Train_path, Test_path):\n",
    "\n",
    "    Train1 = pd.read_csv(Train_path)\n",
    "    Test1 = pd.read_csv(Test_path)\n",
    "    \n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = Test1.dropna()\n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    print(Test1.shape)\n",
    "    print(Train1.shape)\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "\n",
    "    List_nodes =[16,32,48,64,96,128]\n",
    "    List_TimeSteps = [1,3,6,9,12]\n",
    "    #nodes = 16\n",
    "    #timesteps=1 \n",
    "    for nodes in List_nodes:\n",
    "        for timesteps in List_TimeSteps:\n",
    "            kernel_size_= 3\n",
    "            pool_size_ = 3\n",
    "            if(timesteps <= kernel_size_):\n",
    "                print('warning: stepsizes is less or equal than kernel size' )\n",
    "                kernel_size_=max(1,timesteps-1)\n",
    "                pool_size_ =max(1,timesteps-1)\n",
    "            #padding because of timesteps\n",
    "            Train = Fill_data(Train1,mean,timesteps, n_features )\n",
    "            Test  = Fill_data(Test1,mean,timesteps, n_features )\n",
    "\n",
    "            #tranform train and test.\n",
    "            New_data = [Train,Test]\n",
    "            New_data = pd.concat(New_data)\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "            Train =New_data[0:Train.shape[0],:]\n",
    "            Test = New_data[Train.shape[0]:,:]\n",
    "\n",
    "            #print('LSTM nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            X_train, y_train = split_sequences(Train, timesteps)\n",
    "            X_test, y_real = split_sequences(Test, timesteps)\n",
    "\n",
    "            print( 'size after appling timesteps: ',X_train.shape,y_train.shape,X_test.shape, y_real.shape)\n",
    "\n",
    "\n",
    "            my_model = My_model(nodes, X_train, y_train,X_test, y_real,kernel_size_, pool_size_,'3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n",
    "            print('model stop after ',len(my_model.history.history['loss']), ' epochs')\n",
    "\n",
    "            # Plot evaluation\n",
    "            #Plot_evaluation(my_model)\n",
    "\n",
    "            #Evaluate\n",
    "            RMSE, R2 = Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features)\n",
    "\n",
    "            print('CNN nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            print('RMSE: ', RMSE)\n",
    "            print('R2: ', R2)\n",
    "            print('\\n\\n')\n",
    "            import os\n",
    "            os.remove('3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'NO2_Train.csv',data_path+'NO2_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'CO_Train.csv',data_path+'CO_Test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'O3_Train.csv',data_path+'O3_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'PM2_5_Train.csv',data_path+'PM2_5_Test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'SO2_Train.csv',data_path+'SO2_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleRNN\n",
    "# test with two, 3 LSTM layers.\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "#thebest is LSTM with 16 nodes - 1 layer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "#print(tf.__version__)\n",
    "#fix with tf 2.1\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "#process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "#LSTM 16- 1 layer - timestep =1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def My_model(nodes, X_train, y_train,X_test, y_real,model_name ):\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=20,\n",
    "                    restore_best_weights =True)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "                             filepath=model_name, \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True)\n",
    "    #the best model will be used, if you want to store the model, uncomment above.\n",
    "    #mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #                         monitor='val_loss', \n",
    "    #                         save_best_only=True)\n",
    "    callback = [es,mc]\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    " \n",
    "    my_model = tf.keras.Sequential()\n",
    "    my_model.add( tf.keras.layers.SimpleRNN(\n",
    "                            units = nodes,\n",
    "                            input_shape = (X_train.shape[1],X_train.shape[2])))#return_state=True)) #return_sequences=True, \n",
    "    my_model.add(tf.keras.layers.Dropout(0.25))\n",
    "    my_model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    my_model.compile(loss = 'mse',\n",
    "     optimizer = opt, \n",
    "     metrics = ['mean_squared_error'])\n",
    "\n",
    "    #my_model.summary()\n",
    "    my_model.fit(X_train,y_train,\n",
    "                    validation_data=(X_test, y_real),\n",
    "                    batch_size = 500,epochs = 300,\n",
    "                    callbacks=callback, verbose=0)\n",
    "    return my_model\n",
    "# source https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def Fill_data(df,mean,timesteps,n_features):\n",
    "    columns = df.columns\n",
    "    row_stand = df.iloc[0]\n",
    "    for i in range (1,timesteps):\n",
    "        row_temp = np.array([])\n",
    "        for j in range(0,4):\n",
    "            temp = row_stand[j]-i\n",
    "            row_temp = np.append(row_temp,temp)\n",
    "        for j in range(4,n_features):\n",
    "            row_temp = np.append(row_temp,mean[j])\n",
    "\n",
    "        minus = 0\n",
    "        if(row_temp[0]<0): #index\n",
    "            row_temp[0] = 0\n",
    "        if(row_temp[1]<0): #hour\n",
    "            row_temp[1] += 24\n",
    "            minus = 1\n",
    "        if(row_temp[2]<0 or minus == 1): #day of the year\n",
    "            row_temp[2] += 365\n",
    "            if(minus == 1):\n",
    "                row_temp[2] -= 1\n",
    "                minus= 0\n",
    "        if(row_temp[3]<0): #weekday : from 0 to 6\n",
    "            row_temp[4] = 0\n",
    "        row_temp= pd.DataFrame(row_temp.reshape(1, len(row_temp)),columns=columns)\n",
    "        df = pd.concat([row_temp,df],ignore_index=True,axis=0)\n",
    "    return df\n",
    "\n",
    "def Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    y_real = concatenate((X_test,y_real), axis=1)\n",
    "    y_real = scaler.inverse_transform(y_real)\n",
    "    y_real = y_real[:,-1]\n",
    "\n",
    "    RMSE = sqrt(mean_squared_error(y_real, y_pred))\n",
    "    from sklearn.metrics import r2_score\n",
    "    R2 = r2_score(y_real, y_pred)\n",
    "    return RMSE,R2\n",
    "def Plot_evaluation(my_model):\n",
    "    #show evaluation early stop\n",
    "    print(len(my_model.history.history['loss']))\n",
    "    plt.plot(my_model.history.history['loss'], label='train')\n",
    "    plt.plot(my_model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#load train and test data here\n",
    "##process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "def Run(Train_path, Test_path):\n",
    "\n",
    "    Train1 = pd.read_csv(Train_path)\n",
    "    Test1 = pd.read_csv(Test_path)\n",
    "    \n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = Test1.dropna()\n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    print(Test1.shape)\n",
    "    print(Train1.shape)\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "\n",
    "    List_nodes =[16,32,48,64,96,128]\n",
    "    List_TimeSteps = [1,3,6,9,12]\n",
    "    #nodes = 16\n",
    "    #timesteps=1 \n",
    "    for nodes in List_nodes:\n",
    "        for timesteps in List_TimeSteps:\n",
    "\n",
    "            #padding because of timesteps\n",
    "            Train = Fill_data(Train1,mean,timesteps, n_features )\n",
    "            Test  = Fill_data(Test1,mean,timesteps, n_features )\n",
    "\n",
    "            #tranform train and test.\n",
    "            New_data = [Train,Test]\n",
    "            New_data = pd.concat(New_data)\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "            Train =New_data[0:Train.shape[0],:]\n",
    "            Test = New_data[Train.shape[0]:,:]\n",
    "\n",
    "            #print('LSTM nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            X_train, y_train = split_sequences(Train, timesteps, 1)\n",
    "            X_test, y_real = split_sequences(Test, timesteps, 1)\n",
    "\n",
    "            print( 'size after appling timesteps: ',X_train.shape,y_train.shape,X_test.shape, y_real.shape)\n",
    "\n",
    "\n",
    "            #LSTM,\n",
    "            my_model = My_model(nodes, X_train, y_train,X_test, y_real,'3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n",
    "            print('model stop after ',len(my_model.history.history['loss']), ' epochs')\n",
    "\n",
    "            # Plot evaluation\n",
    "            #Plot_evaluation(my_model)\n",
    "\n",
    "            #Evaluate\n",
    "            RMSE, R2 = Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features)\n",
    "\n",
    "            print('SimpleRNN nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            print('RMSE: ', RMSE)\n",
    "            print('R2: ', R2)\n",
    "            print('\\n\\n')\n",
    "            import os\n",
    "            os.remove('3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'NO2_Train.csv',data_path+'NO2_Test.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'CO_Train.csv',data_path+'CO_Test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'O3_Train.csv',data_path+'O3_Test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'PM2_5_Train.csv',data_path+'PM2_5_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'SO2_Train.csv',data_path+'SO2_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BiRNN\n",
    "# test with two, 3 LSTM layers.\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "#thebest is LSTM with 16 nodes - 1 layer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "#print(tf.__version__)\n",
    "#fix with tf 2.1\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "#process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "#LSTM 16- 1 layer - timestep =1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def My_model(nodes, X_train, y_train,X_test, y_real,model_name ):\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=20,\n",
    "                    restore_best_weights =True)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "                             filepath=model_name, \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True)\n",
    "    #the best model will be used, if you want to store the model, uncomment above.\n",
    "    #mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #                         monitor='val_loss', \n",
    "    #                         save_best_only=True)\n",
    "    callback = [es,mc]\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    \n",
    "    my_model = tf.keras.Sequential()\n",
    "    my_model.add(tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            units = nodes,\n",
    "            input_shape = (X_train.shape[1],X_train.shape[2])))\n",
    "        )\n",
    "    my_model.add(tf.keras.layers.Dropout(0.25))\n",
    "    my_model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    my_model.compile(loss = 'mse',\n",
    "     optimizer = opt, \n",
    "     metrics = ['mean_squared_error'])\n",
    "\n",
    "    #my_model.summary()\n",
    "    my_model.fit(X_train,y_train,\n",
    "                    validation_data=(X_test, y_real),\n",
    "                    batch_size = 500,epochs = 300,\n",
    "                    callbacks=callback, verbose=0)\n",
    "    return my_model\n",
    "# source https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def Fill_data(df,mean,timesteps,n_features):\n",
    "    columns = df.columns\n",
    "    row_stand = df.iloc[0]\n",
    "    for i in range (1,timesteps):\n",
    "        row_temp = np.array([])\n",
    "        for j in range(0,4):\n",
    "            temp = row_stand[j]-i\n",
    "            row_temp = np.append(row_temp,temp)\n",
    "        for j in range(4,n_features):\n",
    "            row_temp = np.append(row_temp,mean[j])\n",
    "\n",
    "        minus = 0\n",
    "        if(row_temp[0]<0): #index\n",
    "            row_temp[0] = 0\n",
    "        if(row_temp[1]<0): #hour\n",
    "            row_temp[1] += 24\n",
    "            minus = 1\n",
    "        if(row_temp[2]<0 or minus == 1): #day of the year\n",
    "            row_temp[2] += 365\n",
    "            if(minus == 1):\n",
    "                row_temp[2] -= 1\n",
    "                minus= 0\n",
    "        if(row_temp[3]<0): #weekday : from 0 to 6\n",
    "            row_temp[4] = 0\n",
    "        row_temp= pd.DataFrame(row_temp.reshape(1, len(row_temp)),columns=columns)\n",
    "        df = pd.concat([row_temp,df],ignore_index=True,axis=0)\n",
    "    return df\n",
    "\n",
    "def Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    y_real = concatenate((X_test,y_real), axis=1)\n",
    "    y_real = scaler.inverse_transform(y_real)\n",
    "    y_real = y_real[:,-1]\n",
    "\n",
    "    RMSE = sqrt(mean_squared_error(y_real, y_pred))\n",
    "    from sklearn.metrics import r2_score\n",
    "    R2 = r2_score(y_real, y_pred)\n",
    "    return RMSE,R2\n",
    "def Plot_evaluation(my_model):\n",
    "    #show evaluation early stop\n",
    "    print(len(my_model.history.history['loss']))\n",
    "    plt.plot(my_model.history.history['loss'], label='train')\n",
    "    plt.plot(my_model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#load train and test data here\n",
    "##process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "def Run(Train_path, Test_path):\n",
    "\n",
    "    Train1 = pd.read_csv(Train_path)\n",
    "    Test1 = pd.read_csv(Test_path)\n",
    "    \n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = Test1.dropna()\n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    print(Test1.shape)\n",
    "    print(Train1.shape)\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "\n",
    "    List_nodes =[16,32,48,64,96,128]\n",
    "    List_TimeSteps = [1,3,6,9,12]\n",
    "    #nodes = 16\n",
    "    #timesteps=1 \n",
    "    for nodes in List_nodes:\n",
    "        for timesteps in List_TimeSteps:\n",
    "\n",
    "            #padding because of timesteps\n",
    "            Train = Fill_data(Train1,mean,timesteps, n_features )\n",
    "            Test  = Fill_data(Test1,mean,timesteps, n_features )\n",
    "\n",
    "            #tranform train and test.\n",
    "            New_data = [Train,Test]\n",
    "            New_data = pd.concat(New_data)\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "            Train =New_data[0:Train.shape[0],:]\n",
    "            Test = New_data[Train.shape[0]:,:]\n",
    "\n",
    "            #print('BiRNN nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            X_train, y_train = split_sequences(Train, timesteps, 1)\n",
    "            X_test, y_real = split_sequences(Test, timesteps, 1)\n",
    "\n",
    "            print( 'size after appling timesteps: ',X_train.shape,y_train.shape,X_test.shape, y_real.shape)\n",
    "\n",
    "\n",
    "            #BiRNN,\n",
    "            my_model = My_model(nodes, X_train, y_train,X_test, y_real,'3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n",
    "            print('model stop after ',len(my_model.history.history['loss']), ' epochs')\n",
    "\n",
    "            # Plot evaluation\n",
    "            #Plot_evaluation(my_model)\n",
    "\n",
    "            #Evaluate\n",
    "            RMSE, R2 = Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features)\n",
    "\n",
    "            print('BiRNN nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            print('RMSE: ', RMSE)\n",
    "            print('R2: ', R2)\n",
    "            print('\\n\\n')\n",
    "            \n",
    "            import os\n",
    "            os.remove('3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'NO2_Train.csv',data_path+'NO2_Test.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'CO_Train.csv',data_path+'CO_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'O3_Train.csv',data_path+'O3_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'PM2_5_Train.csv',data_path+'PM2_5_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'SO2_Train.csv',data_path+'SO2_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#GRU\n",
    "# test with two, 3 LSTM layers.\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "#thebest is LSTM with 16 nodes - 1 layer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "#print(tf.__version__)\n",
    "#fix with tf 2.1\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "#process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "#LSTM 16- 1 layer - timestep =1\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def My_model(nodes, X_train, y_train,X_test, y_real,model_name ):\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=20,\n",
    "                    restore_best_weights =True)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "                             filepath=model_name, \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True)\n",
    "    #the best model will be used, if you want to store the model, uncomment above.\n",
    "    #mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #                         monitor='val_loss', \n",
    "    #                         save_best_only=True)\n",
    "    callback = [es,mc]\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "\n",
    "    my_model = tf.keras.Sequential()\n",
    "    my_model.add(\n",
    "                tf.keras.layers.GRU(\n",
    "                units = nodes,\n",
    "                input_shape = (X_train.shape[1],X_train.shape[2])))#return_state=True)) #return_sequences=True, \n",
    "    my_model.add(tf.keras.layers.Dropout(0.25))\n",
    "    my_model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    my_model.compile(loss = 'mse',\n",
    "     optimizer = opt, \n",
    "     metrics = ['mean_squared_error'])\n",
    "\n",
    "    #my_model.summary()\n",
    "    my_model.fit(X_train,y_train,\n",
    "                    validation_data=(X_test, y_real),\n",
    "                    batch_size = 500,epochs = 300,\n",
    "                    callbacks=callback, verbose=0)\n",
    "    return my_model\n",
    "# source https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "def Fill_data(df,mean,timesteps,n_features):\n",
    "    columns = df.columns\n",
    "    row_stand = df.iloc[0]\n",
    "    for i in range (1,timesteps):\n",
    "        row_temp = np.array([])\n",
    "        for j in range(0,4):\n",
    "            temp = row_stand[j]-i\n",
    "            row_temp = np.append(row_temp,temp)\n",
    "        for j in range(4,n_features):\n",
    "            row_temp = np.append(row_temp,mean[j])\n",
    "\n",
    "        minus = 0\n",
    "        if(row_temp[0]<0): #index\n",
    "            row_temp[0] = 0\n",
    "        if(row_temp[1]<0): #hour\n",
    "            row_temp[1] += 24\n",
    "            minus = 1\n",
    "        if(row_temp[2]<0 or minus == 1): #day of the year\n",
    "            row_temp[2] += 365\n",
    "            if(minus == 1):\n",
    "                row_temp[2] -= 1\n",
    "                minus= 0\n",
    "        if(row_temp[3]<0): #weekday : from 0 to 6\n",
    "            row_temp[4] = 0\n",
    "        row_temp= pd.DataFrame(row_temp.reshape(1, len(row_temp)),columns=columns)\n",
    "        df = pd.concat([row_temp,df],ignore_index=True,axis=0)\n",
    "    return df\n",
    "\n",
    "def Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features):\n",
    "    from numpy import concatenate\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    y_pred = my_model.predict(X_test)\n",
    "\n",
    "    X_test = X_test.reshape((X_test.shape[0], timesteps*(n_features-1)))\n",
    "    #predict values\n",
    "    from numpy import concatenate\n",
    "    X_test = X_test[:, -(n_features-1):]\n",
    "    y_pred = concatenate((X_test,y_pred), axis=1)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_pred = y_pred[:,-1]\n",
    "\n",
    "    y_real = concatenate((X_test,y_real), axis=1)\n",
    "    y_real = scaler.inverse_transform(y_real)\n",
    "    y_real = y_real[:,-1]\n",
    "\n",
    "    RMSE = sqrt(mean_squared_error(y_real, y_pred))\n",
    "    from sklearn.metrics import r2_score\n",
    "    R2 = r2_score(y_real, y_pred)\n",
    "    return RMSE,R2\n",
    "def Plot_evaluation(my_model):\n",
    "    #show evaluation early stop\n",
    "    print(len(my_model.history.history['loss']))\n",
    "    plt.plot(my_model.history.history['loss'], label='train')\n",
    "    plt.plot(my_model.history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#load train and test data here\n",
    "##process data... \n",
    "#Train from beginning to Index<140593\n",
    "#validation: 140593<= Index < 142034\n",
    "#from 142034 is quatifying data .. relaxation is from 143930\n",
    "\n",
    "def Run(Train_path, Test_path):\n",
    "\n",
    "    Train1 = pd.read_csv(Train_path)\n",
    "    Test1 = pd.read_csv(Test_path)\n",
    "    \n",
    "    Train1 = Train1.dropna()\n",
    "    Test1 = Test1.dropna()\n",
    "\n",
    "    n_features=np.shape(Train1)[1] #12 inluding output\n",
    "\n",
    "    print(Test1.shape)\n",
    "    print(Train1.shape)\n",
    "    columns = Train1.columns\n",
    "    mean = Train1.mean(axis=0)\n",
    "\n",
    "\n",
    "    List_nodes =[16,32,48,64,96,128]\n",
    "    List_TimeSteps = [1,3,6,9,12]\n",
    "    #nodes = 16\n",
    "    #timesteps=1 \n",
    "    for nodes in List_nodes:\n",
    "        for timesteps in List_TimeSteps:\n",
    "\n",
    "            #padding because of timesteps\n",
    "            Train = Fill_data(Train1,mean,timesteps, n_features )\n",
    "            Test  = Fill_data(Test1,mean,timesteps, n_features )\n",
    "\n",
    "            #tranform train and test.\n",
    "            New_data = [Train,Test]\n",
    "            New_data = pd.concat(New_data)\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            New_data = scaler.fit_transform(New_data)\n",
    "\n",
    "            Train =New_data[0:Train.shape[0],:]\n",
    "            Test = New_data[Train.shape[0]:,:]\n",
    "\n",
    "            #print('LSTM nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            X_train, y_train = split_sequences(Train, timesteps, 1)\n",
    "            X_test, y_real = split_sequences(Test, timesteps, 1)\n",
    "\n",
    "            print( 'size after appling timesteps: ',X_train.shape,y_train.shape,X_test.shape, y_real.shape)\n",
    "\n",
    "\n",
    "            #LSTM,\n",
    "            my_model = My_model(nodes, X_train, y_train,X_test, y_real,'3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n",
    "            print('model stop after ',len(my_model.history.history['loss']), ' epochs')\n",
    "\n",
    "            # Plot evaluation\n",
    "            #Plot_evaluation(my_model)\n",
    "\n",
    "            #Evaluate\n",
    "            RMSE, R2 = Evaluation(my_model,X_test,y_real,scaler,timesteps,n_features)\n",
    "\n",
    "            print('GRU nodes: ', nodes,' timesteps: ', timesteps)\n",
    "            print('RMSE: ', RMSE)\n",
    "            print('R2: ', R2)\n",
    "            print('\\n\\n')\n",
    "            \n",
    "            import os\n",
    "            os.remove('3 folds_'+str(nodes)+'_'+str(timesteps)+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'NO2_Train.csv',data_path+'NO2_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'CO_Train.csv',data_path+'CO_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'O3_Train.csv',data_path+'O3_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'PM2_5_Train.csv',data_path+'PM2_5_Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= 'C:\\\\data\\\\New_data\\\\'\n",
    "\n",
    "times = 20\n",
    "for j in range(1, times+1):\n",
    "    print('Time ', j)\n",
    "    Run(data_path+'SO2_Train.csv',data_path+'SO2_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
